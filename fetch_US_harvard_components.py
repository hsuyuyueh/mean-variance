#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import re
from datetime import datetime
from urllib.parse import urlparse

import cloudscraper
from bs4 import BeautifulSoup

def fetch_US_harvard_components(cache_dir):
    """
    æŠ“å– Harvard Endowmentï¼ˆç”± Harvard Management Company ç®¡ç†ï¼‰çš„ 13F æŒå€‰æˆåˆ†è‚¡ï¼š
    å›å‚³æ ¼å¼ç‚º [ (ticker, company_name), ... ]
    """
    os.makedirs(cache_dir, exist_ok=True)
    today = datetime.today().strftime("%Y%m%d")
    cache_file = os.path.join(cache_dir, f"US_harvard_components_{today}.json")

    # å¦‚æœç•¶å¤©å·²æœ‰å¿«å–ï¼Œç›´æ¥è®€å–
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            print(f"[å¿«å–] å·²è®€å– {cache_file}")
            return json.load(f)

    url = "https://fintel.io/zh-hant/i/harvard-management"
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        ),
        "Referer": "https://fintel.io/",
        "Accept-Language": "zh-TW,zh;q=0.9"
    }

    # ä½¿ç”¨ cloudscraper ç¹é Cloudflare
    scraper = cloudscraper.create_scraper()
    try:
        resp = scraper.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ä¸‹è¼‰ç¶²é : {e}")
        return []

    soup = BeautifulSoup(resp.text, "lxml")
    components = []

    # æ‰¾å‡ºæ‰€æœ‰åœ¨æˆåˆ†è‚¡ table è£¡çš„è‚¡ç¥¨é€£çµ
    for a in soup.select("tbody#trans13f tr td a[href^='/zh-hant/s/us/']"):
        href = a["href"]
        raw = urlparse(href).path.rstrip("/").split("/")[-1]
        ticker = raw.upper() + ".US"
        text = a.get_text(strip=True)
        m = re.match(r'[^/]+/\s*(?P<name>.+?)[ã€‚]?$', text)
        company = m.group("name").strip() if m else text

        components.append((ticker, company))
        print(f"â†’ {ticker} : {company}")

    # å¯«å…¥å¿«å–
    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(components, f, ensure_ascii=False, indent=2)
    print(f"[å¿«å–] å·²å„²å­˜ Harvard æˆåˆ†è‚¡åˆ° {cache_file}")

    return components

if __name__ == "__main__":
    comps = fetch_US_harvard_components(cache_dir="./cache")
    print(f"\nğŸ“¦ Harvard æˆåˆ†è‚¡å…± {len(comps)} æª”ï¼š")
    for tk, name in comps:
        print(f"{tk} => {name}")
