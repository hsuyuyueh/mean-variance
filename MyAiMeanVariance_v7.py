#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import nest_asyncio
import pandas as pd
import yfinance as yf
import json
import os
import logging
import warnings

# é—œé–‰æ‰€æœ‰ WARNING èˆ‡ä»¥ä¸‹ç­‰ç´šçš„ logging
logging.disable(logging.WARNING)
# å®Œå…¨å¿½ç•¥ä»»ä½• warnings
warnings.filterwarnings("ignore")

# æ¥è‘—å†å»åŒ¯å…¥ matplotlib
import matplotlib.pyplot as plt
import argparse, sys, json
from pathlib import Path
import shutil
import time

from datetime import datetime, timedelta
from pypfopt.risk_models import exp_cov
from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt.objective_functions import L2_reg
from scipy.stats import norm
from tqdm import tqdm

from check_p_of_ticker import check_p_of_ticker
from fetch_0056_components import fetch_0056_components
from fetch_0050_components import fetch_0050_components
from fetch_00713_components import fetch_00713_components
from fetch_US_berkshire_components import fetch_US_berkshire_components
from fetch_US_harvard_components import fetch_US_harvard_components
from fetch_US_SPY_components import fetch_US_SPY_components
from my_ai_module import gpt_contextual_rating

import time, requests
from datetime import date


import collections
from matplotlib.ticker import MaxNLocator
import seaborn as sns
import numpy as np
import os
import re
import inspect

def lineno():
    """å›å‚³å‘¼å«æ­¤å‡½å¼çš„è¡Œè™Ÿ"""
    return inspect.currentframe().f_back.f_lineno

# ç¯„ä¾‹
#print("é€™æ˜¯è¡Œè™Ÿï¼š", lineno())

# ---- 1. è®€å– API Key ----
ALPHA_VANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API_KEY")
TEJ_API_KEY           = os.getenv("TEJ_API_KEY")

# ---- 2. Rateâ€limit æ§åˆ¶ ----
# Alpha Vantage: 5 calls/minute â†’ æ¯ call éœ€ sleep 12s
def throttle_alpha_vantage():
    time.sleep(12)

# TEJ: trial ä¸Šé™ 500 calls/day, paid 2000/day â†’ ç”¨ç°¡å–®çš„ã€Œæ—¥è¨ˆæ•¸å™¨ã€åŠ ä¸Šæœ€å°å»¶é²
TEJ_DAILY_LIMIT = 500  # è‹¥æ‚¨å·²ä»˜è²»ï¼Œè«‹æ”¹ç‚º 2000
TEJ_COUNTER_FILE = "./cache/tej_count.json"

def throttle_tej():
    today = date.today().isoformat()
    # è¼‰å…¥è¨ˆæ•¸å™¨
    if os.path.exists(TEJ_COUNTER_FILE):
        with open(TEJ_COUNTER_FILE, "r") as f:
            cnt = json.load(f)
    else:
        cnt = {}
    used = cnt.get(today, 0)
    if used >= TEJ_DAILY_LIMIT:
        raise RuntimeError(f"ä»Šæ—¥ TEJ API å‘¼å«æ¬¡æ•¸å·²é”ä¸Šé™ ({TEJ_DAILY_LIMIT})")
    # æ›´æ–°ä¸¦å„²å­˜
    cnt[today] = used + 1
    with open(TEJ_COUNTER_FILE, "w") as f:
        json.dump(cnt, f)
    # æ¯æ¬¡å‘¼å«å°å»¶é²ï¼Œé¿å… burst
    time.sleep(1)

def fetch_fundamentals_yahoo(ticker):
    """
    ä½¿ç”¨ yfinance å¾ Yahoo Finance å–å¾—åŸºæœ¬é¢ï¼šPEã€ROEï¼Œä¸¦è‡ªå‹•è¨ˆç®—ç‡Ÿæ”¶å¹´å¢ç‡ï¼ˆè‹¥å¯ç”¨ï¼‰ã€‚
    """
    tk = yf.Ticker(ticker)
    info = tk.info

    # å–å¾— PEã€ROE
    pe = info.get("trailingPE") or info.get("forwardPE") or 0
    roe = info.get("returnOnEquity", 0) * 100

    # ç‡Ÿæ”¶å¹´å¢ç‡ï¼šå˜—è©¦å¾å¹´åº¦è²¡å ±è¨ˆç®—
    rev_growth = 0
    try:
        fin = tk.financials  # DataFrameï¼Œæ¬„ä½ç‚ºå¹´åº¦
        revenues = fin.loc["Total Revenue"]
        # å–æœ€è¿‘å…©å¹´æ¯”è¼ƒ
        rev_growth = ((revenues.iloc[0] - revenues.iloc[1]) / revenues.iloc[1]) * 100
    except Exception as e:
        rev_growth = 0
        print(date, "lineï¼š", lineno(), e)

    return {
        "pe": float(pe),
        "roe": float(roe),
        "rev_growth": float(rev_growth)
    }
    
def fetch_fundamentals_tej(ticker):
    """
    å¾ TEJ API å–åŸºæœ¬é¢ï¼šPE, ROE, ç‡Ÿæ”¶æˆé•·ç‡ç­‰
    å›å‚³ dict {'pe': float, 'roe': float, 'rev_growth': float, ...}
    """
    throttle_tej()
    url = f"https://api.tej.com.tw/v1/data/{ticker}/fundamental"
    headers = {"Authorization": f"Bearer {TEJ_API_KEY}"}
    resp = requests.get(url, headers=headers, timeout=10)
    data = resp.json().get("data", [{}])[0]
    return {
        'pe':         float(data.get("trailingPE", 0)),
        'roe':       float(data.get("returnOnEquity", 0)) * 100,
        'rev_growth': float(data.get("revenueGrowth", 0)) * 100
    }

def fetch_fundamentals_alpha_vantage(ticker):
    """
    å¾ Alpha Vantage å–åŸºæœ¬é¢ (Company Overview endpoint)
    """
    throttle_alpha_vantage()
    url = (
      "https://www.alphavantage.co/query"
      f"?function=OVERVIEW&symbol={ticker}&apikey={ALPHA_VANTAGE_API_KEY}"
    )
    resp = requests.get(url, timeout=10)
    info = resp.json()
    return {
        'pe':         float(info.get("PERatio", 0)),
        'roe':       float(info.get("ReturnOnEquityTTM", 0)),
        'rev_growth': None  # AV Overview ç„¡æä¾›ï¼Œæˆ–å¯å¦ call TIME_SERIES_CUSTOM
    }
    
# --- add this block (after existing imports) -----------------

def _detect_market(self):
    if all(tk.endswith('.TW') for tk in self.tickers):  return 'TW'
    if all(tk.endswith('.T')  for tk in self.tickers):  return 'JP'
    return 'US'

def _normalize_us_ticker(tk: str) -> str:
    """
    1. åˆªæ‰ MoneyDJ æœƒé™„çš„ '.US'
    2. æŠŠ BRK.B â†’ BRK-B é€™ç¨®é¡è‚¡åˆ¥æ”¹æˆ Yahoo ç”¨çš„ '-'
    """
    if tk.endswith(".US"):
        tk = tk[:-3]
    return tk.replace(".", "-")
# -------------------------------------------------------------

nest_asyncio.apply()

# â”€â”€ è‚¡åƒ¹å–å¾—å‚™æ´ï¼šYahooã€Alpha Vantage + SQLite å¿«å– â”€â”€
import time, sqlite3, requests
from pandas_datareader import data as pdr

DB_PATH = "./cache/price_cache.sqlite3"
# ç¢ºä¿ cache è³‡æ–™å¤¾å­˜åœ¨ï¼Œé¿å…ç„¡æ³•é–‹å•Ÿè³‡æ–™åº«
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
conn = sqlite3.connect(DB_PATH)  # ç¾åœ¨å°±èƒ½æ­£å¸¸å»ºç«‹æˆ–é–‹å•Ÿæª”æ¡ˆäº†
conn.execute("""
CREATE TABLE IF NOT EXISTS price(
    symbol TEXT, date TEXT, adj_close REAL,
    PRIMARY KEY(symbol, date)
)
""")
conn.commit()

def fetch_price_yahoo(symbol, start, end):
    try:
        df = pdr.get_data_yahoo(symbol, start=start, end=end)
        return df["Adj Close"]
    except Exception as e:
        print(date, "lineï¼š", lineno(), e)
        return None

def fetch_price_alphaav(symbol, start, end, api_key, pause=12):
    # å…ˆæŸ¥ SQLite å¿«å–
    cur = conn.execute(
        "SELECT date, adj_close FROM price WHERE symbol=? AND date BETWEEN ? AND ?",
        (symbol, start, end)
    )
    rows = cur.fetchall()
    if rows:
        s = pd.Series({r[0]: r[1] for r in rows})
        return s.sort_index()

    # å‘¼å« Alpha Vantage
    resp = requests.get(
        "https://www.alphavantage.co/query",
        params={
            "function": "TIME_SERIES_DAILY_ADJUSTED",
            "symbol": symbol,
            "apikey": api_key,
            "outputsize": "full"
        },
        timeout=10
    )
    data = resp.json()
    ts = data.get("Time Series (Daily)")
    if not ts:
        print(f"[è­¦å‘Š] Alpha Vantage ç„¡æ—¥ç·šè³‡æ–™ for {symbol}: {data.get('Note') or data.get('Error Message')}")
        return pd.Series(dtype=float)

    df = pd.DataFrame.from_dict(ts, orient="index")
    df.index = pd.to_datetime(df.index)

    # æ‰¾å‡ºå« â€œadjusted closeâ€ çš„æ¬„ä½
    cols = [c for c in df.columns if "adjusted close" in c.lower()]
    if not cols:
        print(f"[è­¦å‘Š] æ‰¾ä¸åˆ°èª¿æ•´å¾Œæ”¶ç›¤åƒ¹æ¬„ä½ for {symbol}ï¼ŒAvailable: {df.columns.tolist()}")
        return pd.Series(dtype=float)

    s = df[cols[0]].loc[start:end].astype(float)

    # å¯«å…¥å¿«å–
    for dt, val in s.items():
        conn.execute(
            "INSERT OR IGNORE INTO price(symbol,date,adj_close) VALUES(?,?,?)",
            (symbol, dt.strftime("%Y-%m-%d"), float(val))
        )
    conn.commit()
    time.sleep(pause)
    return s

def fetch_price(symbols, start, end, av_api_key=None):
    """ä¸€æ¬¡ä¸‹è¼‰æ‰€æœ‰ ticker çš„åƒ¹æ ¼ï¼Œå›å‚³ Adj Close DataFrame"""
    data = yf.download(
        tickers=symbols,
        start=start,
        end=end,
        auto_adjust=True,
        progress=False
    )
    # å¦‚æœæœ‰å¤šå±¤æ¬„ä½ï¼Œå– Adj Close
    return data['Adj Close'] if 'Adj Close' in data else data


class AiMeanVariancePortfolio:
    def __init__(self, tickers, prices, market, OUTPUT_ROOT, RUN_DATE, profile_level='P3'):
        self.profile_level = profile_level
        self.tickers = tickers
        self.prices = prices
        self.market  = market          # NEW
        self.mu_final = None
        self.fundamentals = None
        self.rf_rate = 0.015
        self.weights = None
        self.performance = None
        self.target_return = None
        self.OUTPUT_ROOT = OUTPUT_ROOT
        self.RUN_DAT = RUN_DATE
        
        ## 1. åˆ—å‡ºè¦é æ¸¬çš„æ‰€æœ‰ tickers
        #print("Component tickers:", self.tickers)
        #
        ## 2. mu_final ä¾†æºï¼šAI è³‡æ–™å¤¾è£¡æœ‰å“ªäº› mu-*.txt
        #ai_dir = f"./outputs/20250721/TW/AI"
        #mu_files = [f for f in os.listdir(ai_dir) if f.startswith("mu-") and f.endswith(".txt")]
        #print("å·²ç”¢ç”Ÿçš„ Î¼ æª”æ¡ˆï¼š", [f.split('-')[1].split('.')[0] for f in mu_files])
        #
        ## 3. åƒ¹æ ¼è³‡æ–™ï¼šself.prices.columns
        #model._load_prices()  # æˆ–ç›´æ¥æª¢æŸ¥ model.prices
        #print("åƒ¹æ ¼è³‡æ–™æ¬„ä½ï¼š", list(model.prices.columns))
        #
        ## æ‰¾å‡ºç¼ºå°‘çš„ ticker
        #missing_mu = set(model.tickers) - set(f.split('-')[1].split('.')[0] for f in mu_files)
        #missing_price = set(model.tickers) - set(model.prices.columns)
        #print("ç¼ºå°‘ Î¼ é æ¸¬æª”æ¡ˆçš„ tickersï¼š", missing_mu)
        #print("ç¼ºå°‘åƒ¹æ ¼è³‡æ–™çš„ tickersï¼š", missing_price)

    def fetch_fundamentals(self):
        cache_file = os.path.join(self.OUTPUT_ROOT, f"fundamentals_{self.RUN_DAT}.json")
        if os.path.exists(cache_file):
            print(f"[å¿«å–] è¼‰å…¥åŸºæœ¬é¢è³‡æ–™ï¼š{cache_file}")
            self.fundamentals = pd.read_json(cache_file)
            return

        fundamentals = {}
        for tk in tqdm(self.tickers, desc="æŠ“å–åŸºæœ¬é¢è³‡æ–™"):
            try:
                if self.market == 'TW':
                    info = fetch_fundamentals_yahoo(tk)
                else:
                    # JP å¯å¥—ç”¨ Alpha Vantage æˆ–ä¿ç•™åŸ yfinance
                    info = fetch_fundamentals_yahoo(tk)
                fundamentals[tk] = info
            except Exception as e:
                print(date, "lineï¼š", lineno(), e)
                print(f"[è­¦å‘Š] {tk} åŸºæœ¬é¢æŠ“å–å¤±æ•—ï¼š{e}ï¼Œä½¿ç”¨é è¨­å€¼")
                fundamentals[tk] = {'pe':20, 'roe':10, 'rev_growth':0}
            time.sleep(0.2)

        df = pd.DataFrame.from_dict(fundamentals, orient='index')
        df.to_json(cache_file, force_ascii=False, indent=2)
        print(f"[å¿«å–] å·²å„²å­˜åŸºæœ¬é¢è³‡æ–™è‡³ï¼š{cache_file}")
        self.fundamentals = df

    def build_mu(self):
        import os, json, pandas as pd, numpy as np

        # 1. çµ±ä¸€å¿«å–æª”åï¼ˆç„¡æ—¥æœŸå¾Œç¶´ï¼‰
        cache_file = os.path.join(self.OUTPUT_ROOT, "default_mu_cache.json")

        # --- å…ˆè®€å¿«å–ï¼šè‹¥æª”æ¡ˆå­˜åœ¨å°±ç›´æ¥è¼‰å…¥ä¸¦å›å‚³ ---
        if os.path.exists(cache_file):
            with open(cache_file, "r", encoding="utf-8") as cf:
                mu_dict = json.load(cf)
            # åŠ ä¸Š market å¾Œç¶´
            self.mu_final = pd.Series({
                f"{ticker}.{self.market}": float(val)
                for ticker, val in mu_dict.items()
            })
            print(f"[INFO] å·²å¾å¿«å–è®€å– Î¼ï¼š{cache_file}")
            return

        # 2. Step 1: æœ¬åœ°è¨ˆç®— historical Î¼
        log_ret = np.log(self.prices / self.prices.shift(1)).dropna()
        mu_local = log_ret.mean() * 252
        local_mu_path = os.path.join(self.OUTPUT_ROOT, f"local_mu_{self.RUN_DAT}.json")
        mu_local.to_json(local_mu_path, force_ascii=False, indent=2)
        print(f"[INFO] å·²å„²å­˜æœ¬åœ° historical Î¼ è‡³ {local_mu_path}")

        # 3. Step 2: è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ (ç•¥ï¼ŒåŒæ‚¨åŸæœ¬ç¨‹å¼)
        # â€”â€” Step 2: æœ¬åœ°è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ â€”â€” 
        if self.prices.empty:
            tech_indicators = {
                "ma5":       {},
                "macd":      {},
                "kd_k":      {},
                "kd_d":      {},
                "year_line": {},
            }
        else:
            adj = self.prices
            ma5   = adj.rolling(window=5).mean().iloc[-1]
            ema12 = adj.ewm(span=12).mean()
            ema26 = adj.ewm(span=26).mean()
            macd  = (ema12 - ema26).iloc[-1]
            low9  = adj.rolling(9).min()
            high9 = adj.rolling(9).max()
            raw_k = (adj - low9) / (high9 - low9) * 100
            kd_k  = raw_k.iloc[-1]
            kd_d  = raw_k.rolling(window=3).mean().iloc[-1]
            year_line = adj.rolling(window=252).mean().iloc[-1]

            tech_indicators = {
                "ma5":       ma5.round(2).to_dict(),
                "macd":      macd.round(4).to_dict(),
                "kd_k":      kd_k.round(2).to_dict(),
                "kd_d":      kd_d.round(2).to_dict(),
                "year_line": year_line.round(2).to_dict(),
            }
            
        # 4. Step 3: å‘¼å« AI
        self.mu_final = gpt_contextual_rating(
            tickers=self.tickers,
            base_mu=mu_local.to_dict(),
            tech_indicators=tech_indicators,
            force=False,
            OUTPUT_ROOT=self.OUTPUT_ROOT
        )

        # 5. Fallback: cache èˆ‡ AI txt çš†ç„¡ï¼Œå¼·è¡Œå¾ AI è³‡æ–™å¤¾è®€å–
        if self.mu_final.empty:
            print("[è­¦å‘Š] cache ç©ºæª”ï¼Œæ”¹å¾ AI è³‡æ–™å¤¾è®€å– mu-*.txt")
            ai_folder = os.path.join(self.OUTPUT_ROOT, "AI")
            mu_dict = {}
            for fn in os.listdir(ai_folder):
                if fn.startswith("mu-") and fn.endswith(".txt"):
                    ticker = fn.split("-", 1)[1].split(".")[0] + "." + self.market
                    
                    
                with open(os.path.join(ai_folder, fn), "r", encoding="utf-8") as cf:
                    content = cf.read()
                    # çµ±ä¸€ JSON æ“·å–æµç¨‹
                    pattern = r'(?s)^.*?(?=== RAW CONTENT ===)'
                    content = re.sub(pattern, '', content)
                    json_match = re.search(r"```json\s*([\s\S]*?)\s*```", content)
                    if json_match and json_match.group(1).strip():
                        json_str = json_match.group(1).strip()
                    else:
                        obj_match = re.search(r"(\{[\s\S]*?\})", content)
                        if obj_match:
                            json_str = obj_match.group(1)
                        else:
                            raise ValueError("ç„¡æ³•å¾ AI å›æ‡‰ä¸­æ“·å–åˆ° JSON ç‰©ä»¶")
                    print(f"\n\n\n[å¿«å–] {json_str}")
                    data = json.loads(json_str)
                    mu_raw = float(data.get("mu_prediction", 0.0))
                    mu_dict[ticker] = mu_raw / 100 if mu_raw > 1 else mu_raw
                    continue
                    

            self.mu_final = pd.Series(mu_dict)

        # 6. æœ€å¾Œå¯«å…¥çµ±ä¸€å¿«å–
        with open(cache_file, "w", encoding="utf-8") as cf:
            json.dump({k.split(f".{self.market}")[0]: v
                       for k, v in self.mu_final.to_dict().items()},
                      cf, indent=2)
        print(f"[INFO] å·²å¯«å…¥ Î¼ å¿«å–ï¼š{cache_file}")

    def optimize(self):
        sigma = exp_cov(self.prices, span=180)
        # åŠ ä¸Šä¸€é» jitterï¼Œç¢ºä¿ covariance matrix æ­£å®š
        import numpy as np
        sigma += np.eye(sigma.shape[0]) * 1e-4

        # â€”â€”â€” ä»¥ self.tickers ç‚ºåŸºæº–ï¼Œç¯©é¸åŒæ™‚å­˜åœ¨æ–¼ mu_finalã€covarianceã€èˆ‡åŸå§‹ ticker list â€”â€”â€”
        candidates = [tk for tk in self.tickers
                      if tk in self.mu_final.index
                      and tk in sigma.index]

        if not candidates:
            print("[éŒ¯èª¤] ç„¡æ³•é€²è¡Œæœ€ä½³åŒ–ï¼šmu èˆ‡ cov çš„äº¤é›†ç‚ºç©ºã€‚")
            print(f"  â€¢ åŸå§‹ tickers: {self.tickers}")
            print(f"  â€¢ mu_final.index: {list(self.mu_final.index)}")
            print(f"  â€¢ covariance.index: {list(sigma.index)}")
            print("lineï¼š", lineno())
            return

        # åªå–æœ‰å…±é€šçš„ ticker é€²è¡Œæœ€ä½³åŒ–
        mu = self.mu_final.loc[candidates]
        sigma = sigma.loc[candidates, candidates]
        
        ef1 = EfficientFrontier(mu, sigma, weight_bounds=(0, 1))
        # å˜—è©¦ä»¥ä½¿ç”¨è€…è¨­å®šçš„ç„¡é¢¨éšªåˆ©ç‡è¨ˆç®—æœ€å¤§åŒ– Sharpe çµ„åˆ
        try:
            ef1.max_sharpe(risk_free_rate=self.rf_rate)
        except ValueError as e:
            # ç•¶æ‰€æœ‰è³‡ç”¢é æœŸå ±é…¬éƒ½ â‰¤ ç„¡é¢¨éšªåˆ©ç‡æ™‚ï¼Œæ”¹ç”¨ç„¡é¢¨éšªåˆ©ç‡=0 é‡ç®—
            print(f"[è­¦å‘Š] {e}ï¼Œæ”¹ç”¨ç„¡é¢¨éšªåˆ©ç‡=0 é‡æ–°è¨ˆç®— max_sharpe")
            print(date, "lineï¼š", lineno(), e)
            ef1.max_sharpe(risk_free_rate=0)
        # å–å¾—è©²çµ„åˆçš„é æœŸå¹´åŒ–å ±é…¬
        ret1, _, _ = ef1.portfolio_performance(risk_free_rate=self.rf_rate, verbose=False)
        
        self.target_return = min(ret1, mu.max() * 0.999)
        ef2 = EfficientFrontier(mu, sigma, weight_bounds=(0, 1))
        ef2.add_objective(L2_reg, gamma=0.1)
        ef2.efficient_return(target_return=self.target_return)
        self.weights = ef2.clean_weights()
        self.performance = ef2.portfolio_performance(risk_free_rate=self.rf_rate, verbose=False)

        beta = self.estimate_portfolio_beta()
        print("\n=== æœ€ä½³åŒ–çµæœ (class æ¨¡å¼) ===\n")
        print(f"â†’ æŠ•çµ„ Î²ï¼š{beta}")
        prob = norm.cdf(self.performance[2])
        for tk, w in sorted(self.weights.items(), key=lambda x: -x[1]):
            if w > 0:
                sector = 'N/A'
                try:
                    sector = yf.Ticker(tk).info.get('sector', 'N/A')
                except Exception as e:
                    print(date, "lineï¼š", lineno(), e)
                    pass
                print(f"{tk}: {w:.2%}, Î¼={mu.get(tk,0):.2%}, Sector={sector}")
                args.source = tk
                args.sep='\t'
                args.period='6mo'
                args.window=63
                
                args.Increase=1+(self.performance[1]/2)
                args.T=21
                check_p_of_ticker(args)
                                
                args.Increase=1+(self.performance[1]/2)
                args.T=10
                check_p_of_ticker(args)
                                
                args.Increase=1+(self.performance[1]/2)
                args.T=5
                check_p_of_ticker(args)
                
                args.Increase=1-self.performance[1]
                args.T=63
                check_p_of_ticker(args)
                
                args.Increase=1-(self.performance[1]/2)
                args.T=21
                check_p_of_ticker(args)
                
                args.Increase=1-(self.performance[1]/2)
                args.T=10
                check_p_of_ticker(args)
                
                args.Increase=1-(self.performance[1]/2)
                args.T=5
                check_p_of_ticker(args)
                
                args.Increase=1-(self.performance[1]/2)
                args.T=2
                check_p_of_ticker(args)
                
                args.Increase=1+self.performance[1]
                args.T=63
                check_p_of_ticker(args)

        print(f"\né æœŸå¹´åŒ–å ±é…¬: {self.performance[0]:.2%}, å¹´åŒ–æ³¢å‹•ç‡: {self.performance[1]:.2%}, Sharpe: {self.performance[2]:.2f}, P: {prob:.2f}\n")

    def estimate_portfolio_beta(self):
        idx_map = {'TW':'^TWII', 'US':'^GSPC', 'JP':'^N225'}
        # 1. å…ˆä¸‹è¼‰åŸºæº–æŒ‡æ•¸å ±é…¬
        bm_df = yf.download(idx_map[self.market], period="1y", progress=False, auto_adjust=False)
        if "Adj Close" in bm_df.columns:
            benchmark = bm_df["Adj Close"].pct_change().dropna()
        else:
            benchmark = bm_df["Close"].pct_change().dropna()

        # 2. é‡å°æ¯å€‹è‚¡ç¥¨è¨ˆç®— beta
        betas = {}
        for tk in self.tickers:
            try:
                stk_df = yf.download(tk, period="1y", progress=False, auto_adjust=False)
                if "Adj Close" in stk_df.columns:
                    stk = stk_df["Adj Close"].pct_change().dropna()
                else:
                    stk = stk_df["Close"].pct_change().dropna()
                df2 = pd.concat([stk, benchmark], axis=1).dropna()
                betas[tk] = df2.cov().iloc[0,1] / df2.iloc[:,1].var()
            except Exception as e:
                print(date, "lineï¼š", lineno(), e)
                betas[tk] = 1.0

        # 3. åŠ æ¬Šå¹³å‡æ‰€æœ‰æŒè‚¡çš„ beta
        return round(sum(self.weights.get(tk, 0) * betas.get(tk, 1.0)
                         for tk in self.tickers), 2)

    def save_weights(self, filepath=None):
        path = filepath or os.path.join(OUTPUT_ROOT, "current_portfolio.json")
        with open(path, 'w') as f:
            json.dump(self.weights, f, indent=2)

    def fetch_benchmark_series(self):
        import yfinance as yf
        import pandas as pd

        # ä¸‹è¼‰åŸºæº–è³‡æ–™ï¼Œç¢ºä¿åŒæ™‚å›å‚³ Close èˆ‡ Adj Close
        if self.market == 'TW':
            df_bench = yf.download("0050.TW", period="1y", progress=False, auto_adjust=False)
        else:  # US
            df_bench = yf.download("^GSPC", period="1y", progress=False, auto_adjust=False)

        # å„ªå…ˆä½¿ç”¨ â€˜Adj Closeâ€™ï¼Œè‹¥ç„¡æ­¤æ¬„ä½å‰‡é€€è€Œæ±‚å…¶æ¬¡ä½¿ç”¨ â€˜Closeâ€™
        if 'Adj Close' in df_bench.columns:
            bm = df_bench['Adj Close']
        else:
            bm = df_bench['Close']

        return bm.dropna()
    
    def plot_top_holdings_summary(self, df):
        holding_counter = collections.Counter()
        for top_dict in df['top_holdings']:
            holding_counter.update(top_dict)

        top_items = holding_counter.most_common(10)
        tickers = [k for k, _ in top_items]
        counts = [v for _, v in top_items]

        plt.figure(figsize=(10, 5))
        bars = plt.bar(tickers, counts)
        plt.xlabel("Top Holdings (Most Frequent)")
        plt.ylabel("Appearance Count")
        plt.title("Top Holdings Frequency During Backtest")
        plt.xticks(rotation=45)
        plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
        plt.tight_layout()
        output_path = os.path.join(self.OUTPUT_ROOT, "top_holdings_chart.png")
        plt.savefig(output_path)
        print(f"[è³‡è¨Š] å·²å„²å­˜ top holdings åœ–è¡¨ï¼š{output_path}")

    def plot_sharpe_beta_map(self, df):
        if 'sharpe' in df.columns and 'beta' in df.columns:
            plt.figure(figsize=(8, 6))
            sns.scatterplot(data=df, x='beta', y='sharpe', hue='market_cond', palette='Set2', s=80)
            plt.axhline(0.3, ls='--', c='gray', label='Sharpe Threshold 0.3')
            plt.axvline(1.0, ls='--', c='gray', label='Beta=1')
            plt.xlabel('Portfolio Beta')
            plt.ylabel('Sharpe Ratio')
            plt.title('Sharpe vs Beta by Market Condition')
            plt.legend()
            plt.tight_layout()
            output_path = os.path.join(self.OUTPUT_ROOT, "sharpe_vs_beta_map.png")
            plt.savefig(output_path)
            print(f"[è³‡è¨Š] å·²å„²å­˜ Sharpe vs Beta åœ–è¡¨ï¼š{output_path}")

    def plot_sharpe_winrate_map(self, df):
        if 'sharpe' in df.columns and 'market_cond' in df.columns:
            df_copy = df.copy()
            df_copy['win'] = df_copy['exp_ret'] > df_copy['benchmark_value'].pct_change(periods=1).shift(-1) * 252
            df_copy['win'] = df_copy['win'].astype(int)
            plt.figure(figsize=(8, 6))
            sns.scatterplot(data=df_copy, x='sharpe', y='win', hue='market_cond', palette='coolwarm', s=80)
            plt.xlabel('Sharpe Ratio')
            plt.ylabel('Victory (1=beat benchmark)')
            plt.title('Sharpe vs Victory by Market Condition')
            plt.yticks([0, 1])
            plt.tight_layout()
            output_path = os.path.join(self.OUTPUT_ROOT, "sharpe_vs_victory_map.png")
            plt.savefig(output_path)
            print(f"[è³‡è¨Š] å·²å„²å­˜ Sharpe vs Victory åœ–è¡¨ï¼š{output_path}")

    def _label_market_condition(self, date, vix_series, sp500_series):
        if date not in vix_series.index or date not in sp500_series.index:
            return "unknown"
        vix = vix_series.loc[date]
        sp = sp500_series.loc[date]
        ma200 = sp500_series.rolling(200).mean().loc[date]
        if vix < 15 and sp > ma200:
            return "bull"
        elif vix > 25 and sp < ma200:
            return "bear"
        else:
            return "neutral"

    def generate_html_report(self):
        html_path = Path(self.OUTPUT_ROOT) / "backtest_summary.html"
        with open(html_path, "w", encoding="utf-8") as f:
            f.write("""
            <html><head><meta charset='utf-8'><title>Backtest Summary</title></head><body>
            <h1>ğŸ“Š Backtest Summary</h1>
            <ul>
              <li><img src='top_holdings_chart.png' width='600'></li>
              <li><img src='sharpe_vs_beta_map.png' width='600'></li>
              <li><img src='sharpe_vs_victory_map.png' width='600'></li>
              <li><img src='portfolio_vs_benchmark.png' width='600'></li>
            </ul>
            <p>For full data, see <code>backtest_report.xlsx</code> and <code>backtest_summary.txt</code>.</p>
            </body></html>
            """)
        print(f"[HTML] å·²ç”¢å‡º HTML å ±å‘Šï¼š{html_path}")

    def plot_tracking_error(self, df):
        if 'mu_prediction' in df.columns and 'realized_return' in df.columns:
            df['tracking_error'] = df['mu_prediction'] - df['realized_return']
            plt.figure(figsize=(8, 6))
            sns.scatterplot(x='mu_prediction', y='realized_return', data=df)
            plt.xlabel('Î¼ Prediction')
            plt.ylabel('Realized Return')
            plt.title('Î¼ Prediction vs Realized Return')
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(os.path.join(self.OUTPUT_ROOT, "mu_vs_realized_return.png"))

            plt.figure(figsize=(8, 4))
            sns.histplot(df['tracking_error'], bins=20, kde=True)
            plt.title('Tracking Error Distribution')
            plt.xlabel('Î¼ - Realized Return')
            plt.tight_layout()
            plt.savefig(os.path.join(self.OUTPUT_ROOT, "tracking_error_distribution.png"))

            # summary
            avg_error = df['tracking_error'].mean()
            rmse = np.sqrt(np.mean(df['tracking_error']**2))
            summary_path = os.path.join(self.OUTPUT_ROOT, f"backtest_summary.txt")
            with open(summary_path, 'a', encoding='utf-8') as f:
                f.write(f"\nTracking Error çµ±è¨ˆï¼š\n")
                f.write(f"å¹³å‡èª¤å·®ï¼ˆÎ¼ - å¯¦éš›ï¼‰ï¼š{avg_error:.2%}\n")
                f.write(f"å‡æ–¹æ ¹èª¤å·®ï¼ˆRMSEï¼‰ï¼š {rmse:.2%}\n")


    def plot_weight_change(self, weight_list):
        diffs = []
        for i in range(1, len(weight_list)):
            w1 = weight_list[i-1]
            w2 = weight_list[i]
            tickers = list(set(w1) | set(w2))
            delta = sum(abs(w1.get(tk, 0) - w2.get(tk, 0)) for tk in tickers)
            diffs.append(delta)
        plt.figure(figsize=(10, 4))
        plt.plot(diffs, marker='o')
        plt.title('Portfolio Weight Change Over Time')
        plt.xlabel('Rebalance Step')
        plt.ylabel('Weight Î” (L1 norm)')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.OUTPUT_ROOT, "weight_change_trend.png"))
        print("[è³‡è¨Š] å·²å„²å­˜æ¬Šé‡è®Šå‹•åœ–ï¼šweight_change_trend.png")

    def plot_herfindahl_index(self, df):
        hhi_list = []
        for top_dict in df['top_holdings']:
            weights = np.array(list(top_dict.values()))
            hhi = np.sum(weights**2)
            hhi_list.append(hhi)
        plt.figure(figsize=(10, 4))
        plt.plot(hhi_list, marker='o')
        plt.title('Herfindahl Index (Concentration) Over Time')
        plt.xlabel('Rebalance Step')
        plt.ylabel('HHI')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.OUTPUT_ROOT, "herfindahl_index_trend.png"))
        print("[è³‡è¨Š] å·²å„²å­˜æŒè‚¡é›†ä¸­åº¦åœ–ï¼šherfindahl_index_trend.png")

    def plot_stock_hit_rate(self, df):
        from collections import defaultdict
        hit_counter = defaultdict(lambda: [0, 0])
        for row in df.itertuples():
            for tk, mu in row.top_holdings.items():
                if tk in self.prices.columns:
                    p_start = self.prices[tk].loc[:row.date].iloc[-1]
                    future_window = self.prices[tk].loc[row.date:].head(5)
                    if len(future_window) >= 2:
                        p_end = future_window.iloc[-1]
                        ret = (p_end / p_start - 1) * 252
                        hit = np.sign(mu) == np.sign(ret)
                        hit_counter[tk][0] += int(hit)
                        hit_counter[tk][1] += 1
        tk_list = [k for k, v in hit_counter.items() if v[1] >= 3]
        acc_list = [hit_counter[k][0]/hit_counter[k][1] for k in tk_list]
        plt.figure(figsize=(10, 4))
        plt.bar(tk_list, acc_list)
        plt.ylabel("Hit Rate")
        plt.title("Per-stock Î¼ Direction Accuracy")
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        plt.tight_layout()
        plt.savefig(os.path.join(self.OUTPUT_ROOT, "stock_hit_rate.png"))
        print("[è³‡è¨Š] å·²å„²å­˜å€‹è‚¡æ–¹å‘å‘½ä¸­ç‡åœ–ï¼šstock_hit_rate.png")

    def generate_diagnostic_summary(self, df):
        summary_path = os.path.join(self.OUTPUT_ROOT, "diagnostic_report.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("ğŸ“Š Diagnostic Report for Portfolio Strategy\n")
            f.write("========================================\n\n")
            # === Tracking Error Auto Risk Rule ===
            if 'tracking_error' in df.columns:
                avg_error = df['tracking_error'].mean()
                rmse = np.sqrt(np.mean(df['tracking_error']**2))
                f.write(f"ğŸ“Œ Î¼ é æ¸¬è¿½è¹¤èª¤å·®ï¼š\n  â€¢ å¹³å‡èª¤å·®ï¼š{avg_error:.2%}\n  â€¢ å‡æ–¹æ ¹èª¤å·®ï¼ˆRMSEï¼‰ï¼š{rmse:.2%}\n\n")
                if rmse > 0.04:
                    self.target_return = max(self.rf_rate + 0.005, self.target_return * 0.8)
                    self.risk_control_note = f"âš ï¸ å•Ÿå‹•é¢¨æ§ï¼šTracking Error éé«˜ï¼ˆRMSE = {rmse:.2%}ï¼‰ï¼Œå·²ä¸‹èª¿ç›®æ¨™å ±é…¬è‡³ {self.target_return:.2%}\n"
                    f.write(self.risk_control_note + "\n")
            # === Sharpe Ratio Auto Risk Rule ===
            if 'sharpe' in df.columns and 'beta' in df.columns:
                avg_sharpe = df['sharpe'].mean()
                avg_beta = df['beta'].mean()
                f.write(f"ğŸ“Œ å ±é…¬é¢¨éšªæŒ‡æ¨™ï¼š\n  â€¢ å¹³å‡ Sharpe Ratioï¼š{avg_sharpe:.2f}\n  â€¢ å¹³å‡ Beta å€¼ï¼š{avg_beta:.2f}\n\n")
                if avg_sharpe < 0.2:
                    self.target_return = max(self.rf_rate + 0.002, self.target_return * 0.85)
                    self.risk_control_note2 = f"âš ï¸ å•Ÿå‹•é¢¨æ§ï¼šSharpe Ratio åä½ï¼ˆå¹³å‡ {avg_sharpe:.2f}ï¼‰ï¼Œå·²ä¸‹èª¿ç›®æ¨™å ±é…¬è‡³ {self.target_return:.2%}\n"
                    f.write(self.risk_control_note2 + "\n")
                # === Beta Limit Auto Risk Rule ===
                if avg_beta > 1.3:
                    self.max_beta_limit = 1.0
                    self.risk_control_note3 = f"âš ï¸ å•Ÿå‹•é¢¨æ§ï¼šBeta éé«˜ï¼ˆå¹³å‡ {avg_beta:.2f}ï¼‰ï¼Œå»ºè­°èª¿æ•´æœ€ä½³åŒ–ç›®æ¨™ä»¥é™åˆ¶æ³¢å‹•é¢¨éšª\n"
                    f.write(self.risk_control_note3 + "\n")
            # === HHI Smooth Auto Rule ===
            if 'top_holdings' in df.columns:
                hhi_list = [sum(np.square(list(d.values()))) for d in df['top_holdings']]
                avg_hhi = np.mean(hhi_list)
                f.write(f"ğŸ“Œ çµ„åˆé›†ä¸­åº¦ï¼š\n  â€¢ å¹³å‡ Herfindahl æŒ‡æ•¸ï¼ˆHHIï¼‰ï¼š{avg_hhi:.3f}\n\n")
                if avg_hhi > 0.4:
                    self.hhi_smoothing_enabled = True
                    self.risk_control_note4 = f"âš ï¸ å•Ÿå‹•é¢¨æ§ï¼šçµ„åˆé›†ä¸­åº¦éé«˜ï¼ˆHHI = {avg_hhi:.3f}ï¼‰ï¼Œå»ºè­°ä½¿ç”¨æ¬Šé‡å¹³æ»‘åŒ–è™•ç†\n"
                    f.write(self.risk_control_note4 + "\n")
            if hasattr(self, 'plot_weight_change'):
                f.write(f"ğŸ“Œ æ¬Šé‡ç©©å®šæ€§ï¼š\n  â€¢ å·²ç”Ÿæˆè®Šå‹•åœ–æª”ï¼Œå¯è¦–è¦ºåŒ–èª¿å€‰å¹…åº¦\n\n")
            f.write("ğŸ” å»ºè­°æª¢æŸ¥æŒ‡æ¨™ï¼š\n")
            f.write("  - è‹¥ Tracking Error é«˜ï¼ŒÎ¼ é æ¸¬å¯èƒ½éœ€ä¿®æ­£\n")
            f.write("  - è‹¥ HHI > 0.4ï¼Œçµ„åˆå¯èƒ½éåº¦é›†ä¸­\n")
            f.write("  - è‹¥ Sharpe é«˜ä½† Beta ä¹Ÿé«˜ï¼Œä»£è¡¨é¢¨éšªä¾†è‡ªé«˜æ›éšª\n")
            f.write("  - è‹¥é »ç¹èª¿å€‰ï¼Œè€ƒæ…®åŠ å¼·æ­£å‰‡åŒ–æˆ–æ”¹é€²é æ¸¬ç©©å®šæ€§\n")
            f.write("\nâœ… æœ¬å ±å‘Šå·²æ ¹æ“šæœ€æ–°å›æ¸¬çµæœç”¢ç”Ÿ\n")
        print(f"[è³‡è¨Š] å·²ç”¢å‡ºè¨ºæ–·å ±å‘Šï¼š{summary_path}")

    def fetch_vix_benchmark(self):
        import yfinance as yf
        import pandas as pd

        # å–å¾— VIX èˆ‡ Benchmarkï¼ˆTW:0050, US:^GSPCï¼‰
        if self.market == 'TW':
            df_vix   = yf.download("^VIX",    period="1y", progress=False, auto_adjust=False)
            df_bench = yf.download("0050.TW",  period="1y", progress=False, auto_adjust=False)
        else:
            df_vix   = yf.download("^VIX",   period="1y", progress=False, auto_adjust=False)
            df_bench = yf.download("^GSPC",  period="1y", progress=False, auto_adjust=False)

        vix   = df_vix['Adj Close'] if 'Adj Close' in df_vix.columns else df_vix['Close']
        bench = df_bench['Adj Close'] if 'Adj Close' in df_bench.columns else df_bench['Close']
        return vix.dropna(), bench.dropna()

    def classify_market(vix_value, bench_ret):
        if vix_value > 25 or bench_ret < -0.05:
            return "bear"
        elif vix_value < 15 and bench_ret > 0.05:
            return "bull"
        else:
            return "neutral"
            
## backtest(self, rebalance_freq='2W', window_length=180):
## ğŸ“Œ ä¸€ã€Î¼ é æ¸¬å“è³ªï¼ˆé æ¸¬èƒ½åŠ›ï¼‰
## plot_tracking_error()ï¼šæ¯”è¼ƒ AI é æ¸¬èˆ‡å¯¦éš›å ±é…¬çš„åå·®èˆ‡ç©©å®šæ€§
## âŸ¶ å¯ä»¥è©•ä¼°æ¨¡å‹æ˜¯å¦éåº¦æ¨‚è§€ï¼æ‚²è§€ã€æ˜¯å¦æº–ç¢ºé æ¸¬å ±é…¬è¶¨å‹¢
## 
## plot_stock_hit_rate()ï¼šæ¯æª”å€‹è‚¡ Î¼ æ–¹å‘å‘½ä¸­ç‡
## âŸ¶ æª¢æŸ¥å“ªäº›è‚¡ç¥¨å®¹æ˜“éåº¦é æ¸¬éŒ¯èª¤ï¼Œå¯è€ƒæ…®ã€Œæ’é™¤æŒè‚¡ã€æˆ–åŠ æ¬Šèª¿æ•´
## 
## ğŸ“Œ äºŒã€é¢¨éšªåˆ†æ•£ç¨‹åº¦ï¼ˆé›†ä¸­åº¦é¢¨éšªï¼‰
## plot_herfindahl_index()ï¼šæŒè‚¡é›†ä¸­åº¦æŒ‡æ¨™ï¼ˆHHIï¼‰
## âŸ¶ è‹¥ HHI > 0.4 è¡¨ç¤ºçµ„åˆå¯èƒ½ã€Œå–®å£“ã€æŸå¹¾æª”ï¼Œé•ååˆ†æ•£åŸå‰‡
## 
## ğŸ“Œ ä¸‰ã€ç­–ç•¥ç©©å®šæ€§èˆ‡å¯è§£æ€§
## plot_weight_change()ï¼šæ¯æœŸæ¬Šé‡è®Šå‹•ç¨‹åº¦ï¼ˆL1 normï¼‰
## âŸ¶ è‹¥é »ç¹åŠ‡è®Šï¼Œå¯èƒ½ Î¼ è³‡è¨Šä¸ç©©å®šï¼Œæˆ–æ¨¡å‹åœ¨ç‰¹å®šå¸‚å ´æƒ…æ³éæ–¼æ•æ„Ÿ
## 
## ğŸ“Œ å››ã€Î± èˆ‡é¢¨éšªçš„é—œè¯
## plot_sharpe_beta_map()ï¼šSharpe vs Beta æ•£é»åœ–
## âŸ¶ è©•ä¼°æŠ•è³‡ç¸¾æ•ˆæ˜¯å¦ä¾†è‡ªã€Œé«˜é¢¨éšªæ›é«˜å ±é…¬ã€æˆ–ã€Œæœ‰æ•ˆè³‡è¨Šå£“åˆ¶é¢¨éšªã€
## 
## plot_sharpe_winrate_map()ï¼šSharpe vs å‹ç‡åœ–
## âŸ¶ è‹¥ Sharpe é«˜ä½†å‹ç‡ä½ï¼Œå¯èƒ½å­˜åœ¨ä¸å°ç¨±æç›Šçµæ§‹ï¼ˆè³­ä¸€æ“Šï¼‰
## 
## ğŸ“Œ äº”ã€æœ€å¸¸å‡ºç¾æŒè‚¡çš„è§’è‰²
## plot_top_holdings_summary()
## âŸ¶ å“ªäº›è‚¡ç¥¨æ˜¯å¸¸å‡ºç¾ä¸»åŠ›ï¼Ÿå¯èƒ½æ˜¯ AI ç‰¹åˆ¥åå¥½ï¼ç‰¹åˆ¥æº–çš„æ¨™çš„
## 
## ğŸ“Œ å…­ã€çµæœçš„è¦–è¦ºå ±å‘Šèˆ‡æ±ºç­–åŸºç¤
## HTML å ±å‘Šæ•´åˆï¼šè®“å›æ¸¬çµæœå¯å‚³éçµ¦éæŠ€è¡“åˆ©å®³äººä½¿ç”¨
## âŸ¶ æ”¯æ´ã€Œè³‡æ–™é©…å‹•æ±ºç­–ã€èˆ‡ç­–ç•¥å¯æºé€šæ€§
## 
## ğŸ“Œ ä¸ƒã€æ¨¡å‹æ˜¯å¦éåº¦è¤‡é›œåŒ–ï¼ˆé¢¨æ§è§’åº¦ï¼‰
## Tracking error + weight change + HHI åŒæ™‚åé«˜
## âŸ¶ è¡¨ç¤º Î¼ å¤ªæ¿€é€²ï¼Œæ¨¡å‹è¼¸å‡ºä¸ç©©å®šï¼Œæ‡‰é™éšæˆ–æ­£å‰‡åŒ–æ›´å¼·

    def backtest(self, rebalance_freq='2W', window_length=180):
        results = []
        portfolio_value = [1_000_000]
        dates_bt = []
        benchmark_value = [1_000_000]

        if self.prices.empty:
            print("[éŒ¯èª¤] ç„¡æ³•å›æ¸¬ï¼šåƒ¹æ ¼è³‡æ–™ç‚ºç©ºï¼Œè«‹ç¢ºèªæ˜¯å¦æˆåŠŸå–å¾—è‚¡åƒ¹è³‡æ–™ã€‚")
            return pd.DataFrame()

        vix_series, sp500_series = self.fetch_vix_benchmark()
        benchmark_series = self.fetch_benchmark_series()
        #dates = pd.date_range(self.prices.index[0], self.prices.index[-1], freq=rebalance_freq)
        # å¦‚æœä½ ç”¨ '1M' æœƒå‡ºç¾ FutureWarningï¼Œå¯æ”¹ç”¨ 'ME' (month end)
        freq = rebalance_freq.replace("1M", "ME")
        dates = pd.date_range(self.prices.index[0], self.prices.index[-1], freq=freq)

        win_count = 0
        excess_returns = []

        for date in dates:
            window = self.prices.loc[:date].tail(window_length).dropna(axis=1, how='any')
            # åªåœ¨æ‹¿åˆ°å®Œæ•´ window_length å¤©æ•¸çš„è³‡æ–™æ™‚æ‰åšå›æ¸¬
            if len(window) < window_length:
                print("lineï¼š", lineno())
                continue

            sigma_bt = exp_cov(window, span=180)
            common_bt = self.mu_final.index.intersection(sigma_bt.index)
            mu_bt = self.mu_final.loc[common_bt]
            sigma_bt = sigma_bt.loc[common_bt, common_bt]
            mu_mom = mu_bt * (1 + 0.3 * ((window.iloc[-1]/window.iloc[-60]) - 1))
            vol = window.pct_change().rolling(60).std().mean().fillna(0)
            mu_adj = mu_mom * (1 / (1 + 0.5 * vol))

            try:
                ef_bt = EfficientFrontier(mu_adj, sigma_bt, weight_bounds=(0, 1))
                ef_bt.add_objective(L2_reg, gamma=0.1)

                use_target = self.target_return
                # PyPortfolioOpt æ–°ç‰ˆå·²ç„¡ expected_sharpe()ï¼Œå…ˆå˜—è©¦å‘¼å«ï¼Œè‹¥ä¸å­˜åœ¨å‰‡è·³é
                try:
                    sharpe_est = ef_bt.expected_sharpe()
                except AttributeError as e:
                    sharpe_est = None
                    print(date, "lineï¼š", lineno(), e)
                if sharpe_est is not None and sharpe_est < 0.3:
                    use_target = self.rf_rate + 0.01
                    ef_bt.add_objective(L2_reg, gamma=0.3)

                #ef_bt.efficient_return(target_return=use_target)
                use_target = self.target_return
                # è‹¥ use_target è¶…é mu_adj çš„æœ€å¤§å€¼ï¼Œå‰‡è‡ªå‹•èª¿æ•´ç‚ºæœ€å¤§å¯è¡Œå€¼çš„ 99.9%
                max_ret = mu_adj.max()
                if use_target >= max_ret:
                    print(f"[è­¦å‘Š] target_return {use_target:.4f} â‰¥ æœ€å¤§å¯è¡Œå ±é…¬ {max_ret:.4f}ï¼Œå·²è‡ªå‹•èª¿æ•´")
                    use_target = max_ret * 0.999
                ef_bt.efficient_return(target_return=use_target)
                
                weights_bt = ef_bt.clean_weights()
                perf_bt = ef_bt.portfolio_performance(risk_free_rate=self.rf_rate, verbose=False)
                top5 = dict(sorted(weights_bt.items(), key=lambda x: -x[1])[:5])

                # === è¨ˆç®—è©²æœŸ beta ===
                idx_map = {'TW':'^TWII', 'US':'^GSPC', 'JP':'^N225'}
                #bm_ret = yf.download(idx_map[self.market], period="1y", progress=False)["Adj Close"].pct_change().dropna()
                bm_df = yf.download(idx_map[self.market], period="1y", progress=False, auto_adjust=False)
                if "Adj Close" in bm_df.columns:
                    bm_ret = bm_df["Adj Close"].pct_change().dropna()
                else:
                    bm_ret = bm_df["Close"].pct_change().dropna()
                betas = {}
                for tk in weights_bt:
                    try:
                        stk_ret = yf.download(tk, period="1y", progress=False)["Adj Close"].pct_change().dropna()
                        dfb = pd.concat([stk_ret, bm_ret], axis=1).dropna()
                        beta_val = dfb.cov().iloc[0,1] / dfb.iloc[:,1].var()
                        betas[tk] = beta_val
                    except Exception as e:
                        print(date, "lineï¼š", lineno(), e)
                        betas[tk] = 1.0
                portfolio_beta = round(sum(weights_bt.get(tk, 0) * betas.get(tk, 1.0) for tk in weights_bt), 3)

                Î”t = 14 / 252
                last_value = portfolio_value[-1]
                new_value = last_value * (1 + perf_bt[0] * Î”t)
                portfolio_value.append(new_value)
                dates_bt.append(date)

                if date in benchmark_series.index and date - pd.Timedelta(days=14) in benchmark_series.index:
                    p0 = benchmark_series.loc[date - pd.Timedelta(days=14)]
                    p1 = benchmark_series.loc[date]
                    bm_ret_val = (p1 / p0) - 1
                    benchmark_value.append(benchmark_value[-1] * (1 + bm_ret_val))
                    excess_returns.append(perf_bt[0] - bm_ret_val * 252)
                    if perf_bt[0] > bm_ret_val * 252:
                        win_count += 1
                else:
                    benchmark_value.append(benchmark_value[-1])

                condition = self._label_market_condition(date, vix_series, sp500_series)

                results.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'exp_ret': perf_bt[0],
                    'vol': perf_bt[1],
                    'sharpe': perf_bt[2],
                    'top_holdings': top5,
                    'market_cond': condition,
                    'beta': portfolio_beta
                })

            except Exception as e:
                print(date, "lineï¼š", lineno(), e)
                continue

        df = pd.DataFrame(results)
        if 'market_cond' not in df.columns:
            df['market_cond'] = 'unknown'
        # â€”â€”â€” æ–°å¢ï¼šè‹¥å›æ¸¬çµæœç‚ºç©ºï¼Œè·³éå¾ŒçºŒç¹ªåœ–èˆ‡å ±å‘Šç”Ÿæˆ â€”â€”â€”
        if df.empty:
            print("[è³‡è¨Š] å›æ¸¬çµæœç‚ºç©ºï¼Œç„¡æ³•ç¹ªè£½åœ–è¡¨æˆ–ç”Ÿæˆå ±å‘Šï¼Œå·²è·³éã€‚")
            print("lineï¼š", lineno())
            return df
        if len(portfolio_value) > 1:
            value_series = pd.Series(portfolio_value[1:], index=dates_bt)
            benchmark_series_sim = pd.Series(benchmark_value[1:], index=dates_bt)
            rolling_max = value_series.cummax()
            drawdowns = (value_series - rolling_max) / rolling_max
            max_drawdown = drawdowns.min()
            # ç”¨ index å°é½Š DataFrameï¼Œå†è³¦å€¼
            df['portfolio_value'] = value_series.reindex(df.index)
            df['benchmark_value'] = benchmark_series_sim.reindex(df.index)
            df['drawdown']        = drawdowns.reindex(df.index)
        else:
            max_drawdown = None

        #avg_excess = sum(excess_returns) / len(excess_returns) if excess_returns else 0
        # è®¡ç®—å¹³å‡è¶…é¢æŠ¥é…¬ï¼Œå¹¶ç¡®ä¿ä¸ºæ ‡é‡ï¼ˆä¸æ˜¯ Seriesï¼‰
        raw_excess = sum(excess_returns) / len(excess_returns) if excess_returns else 0
        try:
            # å¦‚æœ accidental å˜æˆäº† Seriesï¼Œå°±å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            avg_excess = float(raw_excess)
        except Exception:
            avg_excess = raw_excess.iloc[0] if hasattr(raw_excess, 'iloc') else raw_excess
        win_rate = win_count / max(len(df),1)

        bt_json = os.path.join(self.OUTPUT_ROOT, "backtest_report.json")
        bt_xlsx = os.path.join(self.OUTPUT_ROOT, "backtest_report.xlsx")
        df.to_json(bt_json, indent=2)
        df.to_excel(bt_xlsx, index=False)

        plt.figure(figsize=(10, 4))
        plt.plot(df['date'], df['portfolio_value'], label='Portfolio')
        plt.plot(df['date'], df['benchmark_value'], label='Benchmark')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(self.OUTPUT_ROOT, "portfolio_vs_benchmark.png"))

        summary_path = os.path.join(self.OUTPUT_ROOT, f"backtest_summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"æœ€å¤§è·Œå¹… Max Drawdown: {max_drawdown:.2%}\n")
            f.write(f"èƒœç‡ï¼ˆæ‰“è´¥åŸºå‡†ï¼‰: {win_count} / {len(df)} = {win_rate:.2%}\n")
            # è¿™é‡Œ avg_excess å·²ç»æ˜¯ floatï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨ç™¾åˆ†æ¯”æ ¼å¼
            f.write(f"å¹³å‡è¶…é¢æŠ¥é…¬: {avg_excess:.2%}\n")

        # plot top holdings chart
        self.plot_top_holdings_summary(df)
        # plot sharpe vs beta
        self.plot_sharpe_beta_map(df)
        # plot sharpe vs victory
        self.plot_sharpe_winrate_map(df)
        # plot tracking error
        self.plot_tracking_error(df)
        # plot weight change
        self.plot_weight_change([row['top_holdings'] for _, row in df.iterrows()])
        # plot herfindahl index
        self.plot_herfindahl_index(df)
        # plot stock Î¼ hit rate
        self.plot_stock_hit_rate(df)
        # generate HTML summary
        self.generate_html_report()
        # generate diagnostic report
        self.generate_diagnostic_summary(df)
        return df


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--market', default='TW', choices=['TW', 'US', 'JP'],
                        help='TW=å°è‚¡ã€US=ç¾è‚¡ã€JP=æ—¥è‚¡')
    
    args = parser.parse_args(sys.argv[1:])
    # è¼¸å‡ºæ ¹ç›®éŒ„ï¼šä½¿ç”¨ç•¶å¤©æ—¥æœŸ
    RUN_DATE = datetime.today().strftime('%Y%m%d')
    OUTPUT_ROOT = os.path.join("./outputs", RUN_DATE)
    OUTPUT_ROOT = os.path.join(OUTPUT_ROOT, args.market)
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    os.environ['OUTPUT_ROOT'] = OUTPUT_ROOT
    DB_PATH = "./cache/price_cache.sqlite3"
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    MARKET_SOURCES = {
        "TW": lambda: [tk for tk, _ in {**dict(fetch_0050_components(OUTPUT_ROOT)),
                                        **dict(fetch_00713_components(OUTPUT_ROOT)),
                                        **dict(fetch_0056_components(OUTPUT_ROOT))}.items()
                       if not tk.startswith("289")],
        "US": lambda: [tk for tk, _ in {**dict(fetch_US_harvard_components(OUTPUT_ROOT)), **dict(fetch_US_berkshire_components(OUTPUT_ROOT))}.items()],
        "JP": lambda: json.load(open("manual_jp_list.json"))  # e.g. ["7203.T","9984.T",...]
                           if os.path.exists("manual_jp_list.json") else ["7203.T","9984.T"]
    }
    tickers = MARKET_SOURCES[args.market]()
    #tickers = ['2330.TW', '2317.TW', '2454.TW']
    #tickers = ['2330.TW', '2317.TW']
    if args.market == "US":                       # åªæ¸…ç¾è‚¡
        tickers = sorted({ _normalize_us_ticker(t) for t in tickers })
    start   = (datetime.today() - timedelta(days=365)).strftime('%Y-%m-%d')
    end     = datetime.today().strftime('%Y-%m-%d')
    
    # æ”¹ç”¨å‚™æ´æ–¹æ¡ˆå–å¾—åƒ¹æ ¼
    if args.market == "TW":
        prices = yf.download(tickers, start=start, end=end, auto_adjust=False)['Adj Close']
    if args.market == "US": 
        prices = yf.download(tickers, start=start, end=end, auto_adjust=False)['Adj Close']
        #prices = fetch_price(
        #    tickers,
        #    start,
        #    end,
        #    av_api_key=ALPHA_VANTAGE_API_KEY
        #).dropna(axis=1, how="all")

    missing = set(tickers) - set(prices.columns)
    if missing:
        print(f"[è­¦å‘Š] ç„¡æ³•å–å¾—ä»¥ä¸‹è‚¡åƒ¹ï¼š{sorted(missing)}")

    
    rf_table = {'TW':0.015, 'US':0.045, 'JP':0.002}
    

    
    model   = AiMeanVariancePortfolio(
                 tickers, prices,
                 market=args.market,              # å‚³é€²å»
                 OUTPUT_ROOT=OUTPUT_ROOT,
                 RUN_DATE=RUN_DATE,
                 profile_level='P3'
              )
    model.rf_rate = rf_table[args.market]         # ä¾å¸‚å ´è¦†å¯«ç„¡é¢¨éšªåˆ©ç‡
    model.fetch_fundamentals()
    model.build_mu()
    # â€”â€” æ–°å¢ Debugï¼šåˆ—å‡ºç¼ºå°‘çš„ Î¼ é æ¸¬èˆ‡åƒ¹æ ¼è³‡æ–™ â€”â€” 
    missing_mu = set(model.tickers) - set(model.mu_final.index)
    missing_price = set(model.tickers) - set(model.prices.columns)

    print("=== Debug: Î¼ èˆ‡åƒ¹æ ¼è³‡æ–™æª¢æŸ¥ ===")
    print("åŸå§‹ tickers         :", model.tickers)
    print("å·²è¼‰å…¥ mu_final.index:", list(model.mu_final.index))
    print("å·²è¼‰å…¥ price columns :", list(model.prices.columns))
    print("ç¼ºå°‘ Î¼ é æ¸¬çš„ tickers:", sorted(missing_mu))
    print("ç¼ºå°‘åƒ¹æ ¼è³‡æ–™çš„ tickers:", sorted(missing_price))
    print("=== End Debug ===")
    model.optimize()
    model.save_weights()
    rebalance_freq="2W"
    print("lineï¼š", lineno())
    df_bt = model.backtest(rebalance_freq=rebalance_freq)
    # ä¿è­·æ€§æª¢æŸ¥ï¼šç¢ºä¿æœ‰å›æ¸¬çµæœä¸”åŒ…å« sharpe æ¬„ä½æ‰åˆ—å°ï¼Œå¦å‰‡æç¤ºéŒ¯èª¤
    if not df_bt.empty and 'sharpe' in df_bt.columns:
        print("\n{rebalance_freq}å›æ¸¬å¹³å‡ Sharpe:", df_bt['sharpe'].mean().round(2))
    else:
        print("\n[éŒ¯èª¤] ç„¡æ³•è¨ˆç®— Sharpeï¼šå›æ¸¬çµæœç‚ºç©ºæˆ–ç¼ºå°‘ 'sharpe' æ¬„ä½ã€‚")
    print("lineï¼š", lineno())
    rebalance_freq="1W"
    df_bt = model.backtest(rebalance_freq=rebalance_freq)
    # ä¿è­·æ€§æª¢æŸ¥ï¼šç¢ºä¿æœ‰å›æ¸¬çµæœä¸”åŒ…å« sharpe æ¬„ä½æ‰åˆ—å°ï¼Œå¦å‰‡æç¤ºéŒ¯èª¤
    if not df_bt.empty and 'sharpe' in df_bt.columns:
        print("\n{rebalance_freq}å›æ¸¬å¹³å‡ Sharpe:", df_bt['sharpe'].mean().round(2))
    else:
        print("\n[éŒ¯èª¤] ç„¡æ³•è¨ˆç®— Sharpeï¼šå›æ¸¬çµæœç‚ºç©ºæˆ–ç¼ºå°‘ 'sharpe' æ¬„ä½ã€‚")
    print("lineï¼š", lineno())
    rebalance_freq="1M"
    df_bt = model.backtest(rebalance_freq=rebalance_freq)
    # ä¿è­·æ€§æª¢æŸ¥ï¼šç¢ºä¿æœ‰å›æ¸¬çµæœä¸”åŒ…å« sharpe æ¬„ä½æ‰åˆ—å°ï¼Œå¦å‰‡æç¤ºéŒ¯èª¤
    if not df_bt.empty and 'sharpe' in df_bt.columns:
        print("\n{rebalance_freq}å›æ¸¬å¹³å‡ Sharpe:", df_bt['sharpe'].mean().round(2))
    else:
        print("\n[éŒ¯èª¤] ç„¡æ³•è¨ˆç®— Sharpeï¼šå›æ¸¬çµæœç‚ºç©ºæˆ–ç¼ºå°‘ 'sharpe' æ¬„ä½ã€‚")
    print("lineï¼š", lineno())
    df_90 = model.backtest(window_length=90)
    print("lineï¼š", lineno())
    df_180 = model.backtest(window_length=180)
    print("lineï¼š", lineno())
    df_252 = model.backtest(window_length=252)
    print("lineï¼š", lineno())
    ## å¦‚æœæœ‰ market_cond æ¬„ä½ï¼Œæ‰åšåˆ†çµ„çµ±è¨ˆï¼›å¦å‰‡æç¤ºè­¦å‘Š
    #if 'market_cond' in df_252.columns:
    #    stats = df_252.groupby('market_cond')[['sharpe', 'drawdown']].mean()
    #    print("[è³‡è¨Š] å„å¸‚å ´ç‹€æ…‹ä¸‹çš„ Sharpe èˆ‡å›æ’¤å¹³å‡å€¼ï¼š")
    #    print(stats)
    #else:
    #    print("[è­¦å‘Š] ç„¡æ³•é€²è¡Œå¸‚å ´ç‹€æ…‹åˆ†çµ„ï¼šDataFrame ä¸­æ²’æœ‰ 'market_cond' æ¬„ä½")
    #df_252.groupby('market_cond')[['sharpe', 'drawdown']].mean()
        
    script_path = sys.argv[0]
    script_name = os.path.basename(script_path)
    script_base = os.path.splitext(script_name)[0]
    # çµæœæª”ç›´æ¥æ”¾åœ¨ OUTPUT_ROOT åº•ä¸‹
    result_filename = f"{script_base}-result.txt"
    result_path = os.path.join(OUTPUT_ROOT, result_filename)
    with open(result_path, 'w', encoding='utf-8') as f:
        f.write("=== æœ€ä½³åŒ–çµæœèˆ‡å›æ¸¬æ‘˜è¦ ===\n")
        # ç¯„ä¾‹ï¼šå¯«å…¥ sharpe å¹³å‡ã€weights JSONã€performance

        # å¯«å…¥çµæœæª”æ™‚ï¼ŒåŒæ¨£æª¢æŸ¥å›æ¸¬è³‡æ–™
        if not df_bt.empty and 'sharpe' in df_bt.columns:
            f.write(f"å›æ¸¬å¹³å‡ Sharpe: {df_bt['sharpe'].mean().round(2)}\n")
        else:
            f.write("å›æ¸¬å¹³å‡ Sharpe: N/A\n")
        f.write("æœ€çµ‚æ¬Šé‡ï¼š\n")
        json.dump(model.weights, f, ensure_ascii=False, indent=2)
        f.write("\nPerformance:\n")
        # å¦‚æœ optimize æœªæˆåŠŸç”¢ç”Ÿ performanceï¼Œå‰‡å¡« Noneï¼Œé¿å… TypeError
        if hasattr(model, 'performance') and model.performance is not None:
            exp_ret, vol, sharpe = model.performance
        else:
            exp_ret = vol = sharpe = None
        json.dump({
            "exp_ret": exp_ret,
            "vol": vol,
            "sharpe": sharpe
        }, f, ensure_ascii=False, indent=2)
    print(f"[è³‡è¨Š] å·²å¯«å…¥çµæœæª”ï¼š{result_path}")
    src_prompt = Path('default_prompt.txt')
    dst_prompt = Path(OUTPUT_ROOT) / 'default_prompt.txt'
    if src_prompt.exists():
        shutil.copy(src_prompt, dst_prompt)
        print(f"å·²è¤‡è£½ default_prompt.txt åˆ° {dst_prompt}")
    else:
        print("è­¦å‘Šï¼šæ‰¾ä¸åˆ° default_prompt.txtï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®")
    